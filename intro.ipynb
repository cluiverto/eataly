{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikoo\\miniconda3\\envs\\primo\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikoo\\miniconda3\\envs\\primo\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "check = \"clui/opus-it-pl-v1\"\n",
    "pipe = pipeline(\"translation_it_to_pl\", model=check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My, którzy jesteśmy dyplomatami, jesteśmy przygotowani, żeby związać się z konfliktymi państwami i problemami o państwachachach.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('Noi che siamo diplomatici , siamo preparati per avere a che fare con conflitti tra stati e problematiche concernenti gli stati',\n",
    "     clean_up_tokenization_spaces=True)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "# Uzyskiwanie wartości zmiennych\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "ACCESS_TOKEN = os.getenv(\"ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by Angelina Mango...\n",
      "\n",
      "Song 1: \"All’alba non si muore\"\n",
      "Song 2: \"​​another world\"\n",
      "Song 3: \"Che t’o dico a fa’\"\n",
      "\n",
      "Reached user-specified song limit (3).\n",
      "Done. Found 3 songs.\n",
      "[Song(id, artist, ...), Song(id, artist, ...), Song(id, artist, ...)]\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius(ACCESS_TOKEN)\n",
    "\n",
    "artist = genius.search_artist(\"Angelina Mango\", max_songs=3, sort=\"title\")\n",
    "print(artist.songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '( Brawa ) ( Brawa ) Nie mam plany , nie mam plany , nie mam plany , nie mam czas , nie mam czas , nie mam czas i nie jest wkrótce , a nie jest późno i nie ma nazwy , a nie ma twarz , nie ma lustra , a tak naprawdę jesteśmy .'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Testo di \"All'alba non si muore\" da anticipazioni]\n",
      "\n",
      "All'alba non si muore\n",
      "Perché non c'è ragione\n",
      "Per provare a dormire\n",
      "Senza poter contare\n",
      "Quanto manca alla fine di questa notte che\n",
      "All'alba, poi, non muore\n",
      "Io voglio respirare\n",
      "E poi voglio invecchiare\n",
      "La vita a volte sembra\n",
      "Soltanto un'impressione\n",
      "Però si fa guardare\n",
      "Quando all'alba non si muore\n",
      "\n",
      "Io non ho più\n",
      "Bisogno di parole\n",
      "La pioggia va piano, poi cade di colpo\n",
      "E non possiamo più viaggiare\n",
      "Né cantare\n",
      "\n",
      "Ah, all'alba non si muore\n",
      "Lo devono inventare\n",
      "Un motivo valido, un dettaglio per scappare\n",
      "Da te che sei l'unica cosa vera\n",
      "Che riesco a cantare\n",
      "E se poi vai via\n",
      "Non riesco a respirare\n",
      "La pioggia va piano, poi cade di colpo\n",
      "E non possiamo più viaggiare\n",
      "Indietro nel tempo\n",
      "Sì, resta solo un grande vuoto\n",
      "Dopo un giorno bello\n",
      "E poi tu sembri proprio\n",
      "Casa mia in cui non entro\n",
      "Casa mia, adesso entro\n"
     ]
    }
   ],
   "source": [
    "song = artist.song(\"All’alba non si muore\")\n",
    "text = song.lyrics\n",
    "# or:\n",
    "# song = genius.search_song(\"To You\", artist.name)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Testo di \"All\\'alba non si muore\" da anticipazioni]\\n\\nAll\\'alba non si muore\\nPerché non c\\'è ragione\\nPer provare a dormire\\nSenza poter contare\\nQuanto manca alla fine di questa notte che\\nAll\\'alba, poi, non muore\\nIo voglio respirare\\nE poi voglio invecchiare\\nLa vita a volte sembra\\nSoltanto un\\'impressione\\nPerò si fa guardare\\nQuando all\\'alba non si muore\\n\\nIo non ho più\\nBisogno di parole\\nLa pioggia va piano, poi cade di colpo\\nE non possiamo più viaggiare\\nNé cantare\\n\\nAh, all\\'alba non si muore\\nLo devono inventare\\nUn motivo valido, un dettaglio per scappare\\nDa te che sei l\\'unica cosa vera\\nChe riesco a cantare\\nE se poi vai via\\nNon riesco a respirare\\nLa pioggia va piano, poi cade di colpo\\nE non possiamo più viaggiare\\nIndietro nel tempo\\nSì, resta solo un grande vuoto\\nDopo un giorno bello\\nE poi tu sembri proprio\\nCasa mia in cui non entro\\nCasa mia, adesso entro'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Żeby spróbować się .'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Per provare a dormire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowany tokenizer: MarianTokenizer\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(check)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(check)\n",
    "\n",
    "print(f\"Załadowany tokenizer: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: [Testo di \"Che t'o dico a fa'\"]\n",
      "Tokens: ['▁[', 'Testo', '▁di', '▁\"', 'Che', '▁t', \"'\", 'o', '▁di', 'co', '▁a', '▁fa', \"'\", '\"', ']']\n",
      "Input IDs: tensor([[  513, 21583,     6,    36, 12664,   651,     5,    33,     6,   237,\n",
      "             9,   245,     5,    96,   650,     0]])\n",
      "\n",
      "Line: [Strofa 1]\n",
      "Tokens: ['▁[', 'S', 'tro', 'fa', '▁1', ']']\n",
      "Input IDs: tensor([[ 513,  140, 1097,  915,  124,  650,    0]])\n",
      "\n",
      "Line: Io non ho piani, io non ho piani\n",
      "Tokens: ['▁Io', '▁non', '▁ho', '▁piani', ',', '▁io', '▁non', '▁ho', '▁piani']\n",
      "Input IDs: tensor([[ 838,   44,  209, 3893,    3,  445,   44,  209, 3893,    0]])\n",
      "\n",
      "Line: E non ho orari, io non ho orari\n",
      "Tokens: ['▁E', '▁non', '▁ho', '▁orari', ',', '▁io', '▁non', '▁ho', '▁orari']\n",
      "Input IDs: tensor([[   59,    44,   209, 10193,     3,   445,    44,   209, 10193,     0]])\n",
      "\n",
      "Line: E non è presto e non è tardi e non ha un nome\n",
      "Tokens: ['▁E', '▁non', '▁è', '▁presto', '▁e', '▁non', '▁è', '▁tardi', '▁e', '▁non', '▁ha', '▁un', '▁nome']\n",
      "Input IDs: tensor([[  59,   44,   32, 3161,   10,   44,   32, 3771,   10,   44,   73,   23,\n",
      "          814,    0]])\n",
      "\n",
      "Line: Questa faccia non ha specchi, tanto siamo uguali\n",
      "Tokens: ['▁Questa', '▁faccia', '▁non', '▁ha', '▁specchi', ',', '▁tanto', '▁siamo', '▁uguali']\n",
      "Input IDs: tensor([[  881,  2248,    44,    73, 20367,     3,  1361,   773, 15013,     0]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podział tekstu na linie\n",
    "lines = text.split('\\n')\n",
    "\n",
    "# Tokenizacja każdej linii\n",
    "for line in lines:\n",
    "    if line.strip():  # Sprawdzenie, czy linia nie jest pusta\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        input_ids = tokenizer(line, return_tensors=\"pt\").input_ids\n",
    "        print(f\"Line: {line}\")\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Input IDs:\", input_ids)\n",
    "        print()  # Pusta linia dla lepszej czytelności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Line: [Testo di \"Che t'o dico a fa'\"]\n",
      "Translated Line: &quot; &quot; Tak mówię &quot; &quot;. &quot; &quot;\n",
      "\n",
      "Original Line: [Strofa 1]\n",
      "Translated Line: ( Strofa 1 )\n",
      "\n",
      "Original Line: Io non ho piani, io non ho piani\n",
      "Translated Line: Nie mam plany. Nie mam plany.\n",
      "\n",
      "Original Line: E non ho orari, io non ho orari\n",
      "Translated Line: Nie mam czas, nie mam czas.\n",
      "\n",
      "Original Line: E non è presto e non è tardi e non ha un nome\n",
      "Translated Line: Nie jest szybko i nie jest późno i nie ma nazw.\n",
      "\n",
      "Original Line: Questa faccia non ha specchi, tanto siamo uguali\n",
      "Translated Line: Ta twarza nie ma lustra, ale jesteśmy takie same.\n",
      "\n",
      "Final Translated Text:\n",
      "&quot; &quot; Tak mówię &quot; &quot;. &quot; &quot;\n",
      "( Strofa 1 )\n",
      "Nie mam plany. Nie mam plany.\n",
      "Nie mam czas, nie mam czas.\n",
      "Nie jest szybko i nie jest późno i nie ma nazw.\n",
      "Ta twarza nie ma lustra, ale jesteśmy takie same.\n"
     ]
    }
   ],
   "source": [
    "# Podział tekstu na linie\n",
    "lines = text.split('\\n')\n",
    "\n",
    "# Tłumaczenie każdej linii\n",
    "translated_lines = []\n",
    "for line in lines:\n",
    "    if line.strip():  # Sprawdzenie, czy linia nie jest pusta\n",
    "        # Tokenizacja\n",
    "        inputs = tokenizer(line, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Generowanie tłumaczenia\n",
    "        translated_outputs = model.generate(**inputs)\n",
    "        \n",
    "        # Dekodowanie przetłumaczonego tekstu\n",
    "        translated_text = tokenizer.decode(translated_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        translated_lines.append(translated_text)\n",
    "        print(f\"Original Line: {line}\")\n",
    "        print(f\"Translated Line: {translated_text}\\n\")\n",
    "\n",
    "# Opcjonalnie: Połączenie przetłumaczonych linii w jeden tekst\n",
    "final_translated_text = \"\\n\".join(translated_lines)\n",
    "print(\"Final Translated Text:\")\n",
    "print(final_translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: [Testo di \"Che t'o dico a fa'\"]\n",
      "Tokens: ['[', 'Testo', 'di', '``', 'Che', 't', \"'\", 'o', 'dico', 'a', 'fa', \"'\", \"''\", ']']\n",
      "\n",
      "Line: [Strofa 1]\n",
      "Tokens: ['[', 'Strofa', '1', ']']\n",
      "\n",
      "Line: Io non ho piani, io non ho piani\n",
      "Tokens: ['Io', 'non', 'ho', 'piani', ',', 'io', 'non', 'ho', 'piani']\n",
      "\n",
      "Line: E non ho orari, io non ho orari\n",
      "Tokens: ['E', 'non', 'ho', 'orari', ',', 'io', 'non', 'ho', 'orari']\n",
      "\n",
      "Line: E non è presto e non è tardi e non ha un nome\n",
      "Tokens: ['E', 'non', 'è', 'presto', 'e', 'non', 'è', 'tardi', 'e', 'non', 'ha', 'un', 'nome']\n",
      "\n",
      "Line: Questa faccia non ha specchi, tanto siamo uguali\n",
      "Tokens: ['Questa', 'faccia', 'non', 'ha', 'specchi', ',', 'tanto', 'siamo', 'uguali']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mikoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Upewnij się, że masz pobrane zasoby NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Przykładowy tekst do tokenizacji\n",
    "text = \"[Testo di \\\"Che t'o dico a fa'\\\"]\\n\\n[Strofa 1]\\nIo non ho piani, io non ho piani\\nE non ho orari, io non ho orari\\nE non è presto e non è tardi e non ha un nome\\nQuesta faccia non ha specchi, tanto siamo uguali\"\n",
    "\n",
    "# Podział tekstu na linie\n",
    "lines = text.split('\\n')\n",
    "\n",
    "# Tokenizacja każdej linii\n",
    "for line in lines:\n",
    "    if line.strip():  # Sprawdzenie, czy linia nie jest pusta\n",
    "        tokens = word_tokenize(line)\n",
    "        print(f\"Line: {line}\")\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print()  # Pusta linia dla lepszej czytelności"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
